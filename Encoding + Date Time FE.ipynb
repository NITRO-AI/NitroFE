{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "def label_encode_dataframe(data,columns=data.columns,target=None):\n",
    "    \n",
    "    try:\n",
    "        data = data[columns]\n",
    "        data.drop([target],inplace=True ,axis=1)\n",
    "    except :\n",
    "        logger.error(f'Problem with label encoding : {e})\n",
    "    train=pd.DataFrame()\n",
    "    label=LabelEncoder()\n",
    "    for c in  X_train.columns:\n",
    "        if(X[c].dtype=='object'):\n",
    "            train[c]=label.fit_transform(X_train[c])\n",
    "        else:\n",
    "            train[c]=X_train[c]\n",
    "    return train        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cyclic Feature\n",
    "def cyclic_features(data,columns=data.columns,target=None):\n",
    "\n",
    "    \n",
    "    try:\n",
    "        X_train_cyclic=data[columns]\n",
    "        data.drop([target],inplace=True ,axis=1)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Problem with cyclic feature building {e})\n",
    "    for col in columns:\n",
    "        X_train_cyclic[col+'_sin']=np.sin((2*np.pi*X_train_cyclic[col])/max(X_train_cyclic[col]))\n",
    "        X_train_cyclic[col+'_cos']=np.cos((2*np.pi*X_train_cyclic[col])/max(X_train_cyclic[col]))\n",
    "    X_train_cyclic=X_train_cyclic.drop(columns,axis=1)\n",
    "    return X_train_cyclic                 \n",
    "                     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hash feature\n",
    "def hashed_features(data,columns=data.columns,target=None):\n",
    "    try:\n",
    "        X_train_hash=data[columns]\n",
    "        data.drop([target],inplace=True ,axis=1)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Problem with hashed feature building {e})\n",
    "    for c in X.columns:\n",
    "        X_train_hash[c]=X[c].astype('str')  \n",
    "    hashing=FeatureHasher(input_type='string')\n",
    "    hashed=hashing.transform(X_train_hash.values)    \n",
    "    return hashed                \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one=OneHotEncoder()\n",
    "\n",
    "one.fit(X)\n",
    "train=one.transform(X)\n",
    "\n",
    "print('train data set has got {} rows and {} columns'.format(train.shape[0],train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stat=X.copy()\n",
    "for c in X_train_stat.columns:\n",
    "    if(X_train_stat[c].dtype=='object'):\n",
    "        X_train_stat[c]=X_train_stat[c].astype('category')\n",
    "        counts=X_train_stat[c].value_counts()\n",
    "        counts=counts.sort_index()\n",
    "        counts=counts.fillna(0)\n",
    "        counts += np.random.rand(len(counts))/1000\n",
    "        X_train_stat[c].cat.categories=counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(train, test, categ_variables):\n",
    "    df_onehot = pd.get_dummies(pd.concat([train[cat_vars], test[cat_vars]]).astype(str))\n",
    "    return df_onehot[:len(train)], df_onehot[len(train):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_target_smooth(data, target, categ_variables, smooth):\n",
    "    \"\"\"    \n",
    "    Apply target encoding with smoothing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pd.DataFrame\n",
    "    target: str, dependent variable\n",
    "    categ_variables: list of str, variables to encode\n",
    "    smooth: int, number of observations to weigh global average with\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    encoded_dataset: pd.DataFrame\n",
    "    code_map: dict, mapping to be used on validation/test datasets \n",
    "    defaul_map: dict, mapping to replace previously unseen values with\n",
    "    \"\"\"\n",
    "    train_target = data.copy()\n",
    "    code_map = dict()    # stores mapping between original and encoded values\n",
    "    default_map = dict() # stores global average of each variable\n",
    "    \n",
    "    for v in categ_variables:\n",
    "        prior = data[target].mean()\n",
    "        n = data.groupby(v).size()\n",
    "        mu = data.groupby(v)[target].mean()\n",
    "        mu_smoothed = (n * mu + smooth * prior) / (n + smooth)\n",
    "        \n",
    "        train_target.loc[:, v] = train_target[v].map(mu_smoothed)        \n",
    "        code_map[v] = mu_smoothed\n",
    "        default_map[v] = prior        \n",
    "    return train_target, code_map, default_map\n",
    "\n",
    "\n",
    "#invocation\n",
    "train_target_smooth, target_map, default_map = encode_target_smooth(train, 'adoptionspeed', cat_vars, 500)\n",
    "test_target_smooth = test.copy()\n",
    "for v in cat_vars:\n",
    "    test_target_smooth.loc[:, v] = test_target_smooth[v].map(target_map[v])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/dustinthewind/making-sense-of-mean-encoding\n",
    "\n",
    "def impact_coding_leak(data, feature, target, n_folds=20, n_inner_folds=10):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    '''\n",
    "    ! Using oof_default_mean for encoding inner folds introduces leak.\n",
    "    \n",
    "    Source: https://www.kaggle.com/tnarik/likelihood-encoding-of-categorical-features\n",
    "    \n",
    "    Changelog:    \n",
    "    a) Replaced KFold with StratifiedFold due to class imbalance\n",
    "    b) Rewrote .apply() with .map() for readability\n",
    "    c) Removed redundant apply in the inner loop\n",
    "    '''\n",
    "    impact_coded = pd.Series()\n",
    "    \n",
    "    oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)\n",
    "    kf = StratifiedKFold(n_splits=n_folds, shuffle=True) # KFold in the original\n",
    "    oof_mean_cv = pd.DataFrame()\n",
    "    split = 0\n",
    "    for infold, oof in kf.split(data[feature], data[target]):\n",
    "\n",
    "        kf_inner = StratifiedKFold(n_splits=n_inner_folds, shuffle=True)\n",
    "        inner_split = 0\n",
    "        inner_oof_mean_cv = pd.DataFrame()\n",
    "        oof_default_inner_mean = data.iloc[infold][target].mean()\n",
    "        \n",
    "        for infold_inner, oof_inner in kf_inner.split(data.iloc[infold], data.loc[infold, target]):\n",
    "            # The mean to apply to the inner oof split (a 1/n_folds % based on the rest)\n",
    "            oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()\n",
    "\n",
    "            # Also populate mapping (this has all group -> mean for all inner CV folds)\n",
    "            inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')\n",
    "            inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)\n",
    "            inner_split += 1\n",
    "\n",
    "        # compute mean for each value of categorical value across oof iterations\n",
    "        inner_oof_mean_cv_map = inner_oof_mean_cv.mean(axis=1)\n",
    "\n",
    "        # Also populate mapping\n",
    "        oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')\n",
    "        oof_mean_cv.fillna(value=oof_default_mean, inplace=True)\n",
    "        split += 1\n",
    "\n",
    "        feature_mean = data.loc[oof, feature].map(inner_oof_mean_cv_map).fillna(oof_default_mean)\n",
    "        impact_coded = impact_coded.append(feature_mean)\n",
    "            \n",
    "    return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean\n",
    "\n",
    "\n",
    "def impact_coding(data, feature, target, n_folds=20, n_inner_folds=10):\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    '''\n",
    "    ! Using oof_default_mean for encoding inner folds introduces leak.\n",
    "    \n",
    "    Changelog:    \n",
    "    a) Replaced KFold with StratifiedFold due to class imbalance\n",
    "    b) Rewrote .apply() with .map() for readability\n",
    "    c) Removed redundant apply in the inner loop\n",
    "    d) Removed global average; use local mean to fill NaN values in out-of-fold set\n",
    "    '''\n",
    "    impact_coded = pd.Series()\n",
    "        \n",
    "    kf = StratifiedKFold(n_splits=n_folds, shuffle=True) # KFold in the original\n",
    "    oof_mean_cv = pd.DataFrame()\n",
    "    split = 0\n",
    "    for infold, oof in kf.split(data[feature], data[target]):\n",
    "\n",
    "        kf_inner = StratifiedKFold(n_splits=n_inner_folds, shuffle=True)\n",
    "        inner_split = 0\n",
    "        inner_oof_mean_cv = pd.DataFrame()\n",
    "        oof_default_inner_mean = data.iloc[infold][target].mean()\n",
    "        \n",
    "        for infold_inner, oof_inner in kf_inner.split(data.iloc[infold], data.loc[infold, target]):\n",
    "                    \n",
    "            # The mean to apply to the inner oof split (a 1/n_folds % based on the rest)\n",
    "            oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()\n",
    "            \n",
    "            # Also populate mapping (this has all group -> mean for all inner CV folds)\n",
    "            inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')\n",
    "            inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)\n",
    "            inner_split += 1\n",
    "\n",
    "        # compute mean for each value of categorical value across oof iterations\n",
    "        inner_oof_mean_cv_map = inner_oof_mean_cv.mean(axis=1)\n",
    "\n",
    "        # Also populate mapping\n",
    "        oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')\n",
    "        oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True) # <- local mean as default\n",
    "        split += 1\n",
    "\n",
    "        feature_mean = data.loc[oof, feature].map(inner_oof_mean_cv_map).fillna(oof_default_inner_mean)\n",
    "        impact_coded = impact_coded.append(feature_mean)\n",
    "    \n",
    "    oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)\n",
    "    return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean\n",
    "\n",
    "def encode_target_cv(data, target, categ_variables, impact_coder=impact_coding):\n",
    "    \"\"\"Apply original function for each <categ_variables> in  <data>\n",
    "    Reduced number of validation folds\n",
    "    \"\"\"\n",
    "    train_target = data.copy() \n",
    "    \n",
    "    code_map = dict()\n",
    "    default_map = dict()\n",
    "    for f in categ_variables:\n",
    "        train_target.loc[:, f], code_map[f], default_map[f] = impact_coder(train_target, f, target)\n",
    "        \n",
    "    return train_target, code_map, default_map\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_encoder(X, target, categorical_vars, encoder,\n",
    "                           model, n_splits=10, **kwargs):\n",
    "    \"\"\"Evaluate perfomance of encoding categorical varaibles with <encoder> by fitting \n",
    "    <model> and measuring average kappa on <n_samples> cross validation.\n",
    "    \n",
    "    Make sure to apply mean encoding only to infold set.\n",
    "            \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pd.DataFrame, train data\n",
    "    target: str, response variable\n",
    "    categorical_vars: list of str, categorical variables to encode\n",
    "    encoder: custom function to apply\n",
    "    model: sklearn model to fit\n",
    "    n_splits: number of cross-validation folds\n",
    "    **kwargs: key-word arguments to encoder\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    metric_cvs: np.array of float, metrics computed on the held-out fold\n",
    "    \"\"\"     \n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=20190301)\n",
    "    metric_cvs = list()\n",
    "    \n",
    "    for fold_idx, val_idx in skf.split(X=X, y=X[target]):\n",
    "        train_fold, valid_fold = X.loc[fold_idx].reset_index(drop=True), \\\n",
    "                                 X.loc[val_idx].reset_index(drop=True)\n",
    "        \n",
    "        # apply encoding to k-th fold and validation set \n",
    "        train_fold, code_map, default_map = encoder(train_fold, target, categorical_vars, **kwargs)\n",
    "        for v in categorical_vars:\n",
    "            valid_fold.loc[:, v] = valid_fold[v].map(code_map[v]).fillna(default_map[v])\n",
    "        \n",
    "        # fit model on training fold\n",
    "        model.fit(train_fold[categorical_vars], train_fold[target])\n",
    "        # predict out-of-fold\n",
    "        oof_pred = model.predict(valid_fold[categorical_vars])\n",
    "        metric_cvs.append(cohen_kappa_score(valid_fold[target], oof_pred, weights='quadratic'))        \n",
    "    return np.array(metric_cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X_target=df_train.copy()\n",
    "X_target['day']=X_target['day'].astype('object')\n",
    "X_target['month']=X_target['month'].astype('object')\n",
    "for col in X_target.columns:\n",
    "    if (X_target[col].dtype=='object'):\n",
    "        target= dict ( X_target.groupby(col)['target'].agg('sum')/X_target.groupby(col)['target'].agg('count'))\n",
    "        X_target[col]=X_target[col].replace(target).values\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DateTime Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Extractions\n",
    "df['only_date'] = df['CreationDate'].dt.date\n",
    "df['time'] = df['CreationDate'].dt.time\n",
    "df['year'] = df['CreationDate'].dt.year.astype('Int16')\n",
    "df['month'] = df['CreationDate'].dt.month.astype('Int8')\n",
    "df['month_name'] = df['CreationDate'].dt.strftime('%B') \n",
    "df['day'] = df['CreationDate'].dt.day.astype('Int8')\n",
    "df['weekday'] = df['CreationDate'].dt.strftime('%A')  \n",
    "df['week_num'] = df['CreationDate'].dt.strftime('%W') #  (Monday as first day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick Difference \n",
    "def months(d1,d2):\n",
    "    d1=pd.to_datetime(d1)\n",
    "    d2=pd.to_datetime(d2)\n",
    "    return (d1.year-d2.year)*12 + (d1.month-d2.month) \n",
    "df['diff_year']=round((pd.to_datetime(df['future_date'])-pd.to_datetime(df['CreationDate'])).dt.days/365).astype(int)\n",
    "df['diff_month']=df.apply(lambda x:months(x.future_date,x.CreationDate),axis=1)\n",
    "df['diff_days']=(pd.to_datetime(df['future_date'])-pd.to_datetime(df['CreationDate'])).dt.days\n",
    "df['diff_hours']=(df['diff_days']*24)+(((pd.to_datetime(df['future_date'])-pd.to_datetime(df['CreationDate'])).dt.seconds)// 3600).astype(int)\n",
    "df['diff_minutes']=(df['diff_days']*24*60)+(((pd.to_datetime(df['future_date'])-pd.to_datetime(df['CreationDate'])).dt.seconds)//60).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quick Sum\n",
    "df['add_days']=df['CreationDate']+timedelta(5)\n",
    "df['add_hours_min']=df['CreationDate']+timedelta(hours=5,minutes=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def weekend(date):\n",
    "    date=pd.to_datetime(date)\n",
    "    if((date.strftime('%w')=='0')|(date.strftime('%w')=='6')):\n",
    "        return \"weekend\"\n",
    "    else:\n",
    "        return \"working day\"\n",
    "    \n",
    "df['weekend_or_not']=df['CreationDate'].apply(lambda x:weekend(x))   \n",
    "\n",
    "def leap(date):\n",
    "    date=pd.to_datetime(date)\n",
    "    if(date.year%4==0):\n",
    "        return \"leap\"\n",
    "    else:\n",
    "        return \"non leap\"\n",
    "    \n",
    "df['leap']=df['CreationDate'].apply(lambda x:leap(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Days difference from next row(Shift)\n",
    "shift_df['diff_from_last_date']=(shift_df['CreationDate']-shift_df['CreationDate'].shift()).dt.days\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
